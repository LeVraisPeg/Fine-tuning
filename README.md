# Fine-Tuning de LLM avec PEFT / LoRA  
**Du Flan-T5 √† Llama 3.2-3B (QLoRA)**

## 1. Objectif du projet
L‚Äôobjectif de ce projet est d‚Äôexplorer et de comparer diff√©rentes strat√©gies de **fine-tuning de grands mod√®les de langage (LLM)** en utilisant des techniques **parameter-efficient** (PEFT), et en particulier **LoRA** et **QLoRA**.

Le projet commence par un mod√®le p√©dagogique (**Flan-T5-base**) pour comprendre les m√©canismes, puis √©volue vers un mod√®le moderne et performant (**Llama 3.2-3B Instruct**) charg√© en **4-bits** afin de rester compatible avec des ressources mat√©rielles limit√©es (Google Colab).

---

## 2. Mod√®les utilis√©s

### Mod√®le 1 ‚Äì Flan-T5-base
- Architecture : **Encoder‚ÄìDecoder (Seq2Seq)**
- Avantages : simplicit√©, rapidit√©, id√©al pour l‚Äôapprentissage
- Limites : performances d√©pass√©es par les mod√®les r√©cents

### Mod√®le 2 ‚Äì Llama 3.2-3B Instruct
- Architecture : **Decoder-only (CausalLM)**
- Chargement quantifi√© **4-bits (bitsandbytes / QLoRA)**
- Entra√Ænement via **LoRA**
- Avantages :
  - performances sup√©rieures,
  - faible empreinte m√©moire,
  - fine-tuning possible sur un GPU standard

---

## 3. Dataset

- **DialogSum** (`knkarthick/dialogsum`)
- T√¢che : **r√©sum√© automatique de dialogues**
- Splits :
  - train
  - validation
  - test
- Pour rester dans des temps acceptables :
  - **100 exemples** pour l‚Äôentra√Ænement
  - **50 exemples** pour validation / test

---

## 4. M√©thodologie

### 4.1 Fine-tuning classique (Flan-T5)
- Fine-tuning complet puis via **PEFT / LoRA**
- Comparaison :
  - mod√®le original
  - mod√®le fine-tun√©
- √âvaluation via **ROUGE**

### 4.2 Fine-tuning moderne (Llama 3.2-3B)
- Quantification **4-bits (QLoRA)** avec `bitsandbytes`
- Gel du mod√®le de base avec `prepare_model_for_kbit_training`
- Ajout d‚Äôadapteurs **LoRA** sur les projections d‚Äôattention :
  - `q_proj`, `k_proj`, `v_proj`, `o_proj`
- Entra√Ænement de **~1‚Äì2 % des param√®tres seulement**

---

## 5. Entra√Ænement

- Framework : **Hugging Face Transformers**
- Optimisation :
  - `paged_adamw_8bit`
  - `gradient_checkpointing`
  - `gradient_accumulation_steps`
- Objectif :
  - r√©duire l‚Äôusage VRAM/RAM,
  - garantir la stabilit√© sur Colab

---

## 6. √âvaluation

### 6.1 M√©trique
- **ROUGE** :
  - ROUGE-1 (unigrammes)
  - ROUGE-2 (bigrams)
  - ROUGE-L / ROUGE-Lsum (structure globale)

### 6.2 R√©sultats cl√©s (Llama 3.2-3B)
- Le mod√®le **PEFT** :
  - am√©liore la **coh√©rence locale** (ROUGE-2),
  - am√©liore l√©g√®rement la **structure globale** (ROUGE-L),
  - peut r√©duire l√©g√®rement le recouvrement lexical (ROUGE-1),
    ce qui est coh√©rent avec une g√©n√©ration plus **abstraite**.

üëâ Les r√©sultats montrent que **PEFT permet une adaptation cibl√©e et efficace**, m√™me sur des mod√®les r√©cents.

---

## 7. Enseignements principaux

- Le **fine-tuning complet** est souvent inutile et co√ªteux.
- **LoRA / QLoRA** permettent :
  - une r√©duction drastique des ressources n√©cessaires,
  - une grande flexibilit√© (changement d‚Äôadapter sans recharger le mod√®le).
- Les m√©triques relatives (en %) peuvent √™tre trompeuses :
  - il est essentiel d‚Äôanalyser aussi les **gains absolus**.
- ROUGE reste une m√©trique limit√©e :
  - une √©valuation humaine ou s√©mantique serait compl√©mentaire.

---

## 8. Limites et perspectives

### Limites
- √âvaluation sur un petit nombre d‚Äôexemples (variance √©lev√©e).
- D√©pendance √† ROUGE (m√©trique lexicale).

### Perspectives
- Entra√Æner sur un plus grand sous-ensemble.
- Tester d‚Äôautres configurations LoRA (rank, modules cibl√©s).
- Comparer avec **Qwen 3 1.7B**.
- Ajouter des m√©triques s√©mantiques (BERTScore).

---

## 9. Conclusion
Ce projet d√©montre qu‚Äôil est possible de **fine-tuner efficacement des LLM modernes** avec des ressources limit√©es gr√¢ce √† **PEFT / QLoRA**, tout en obtenant des gains mesurables sur une t√¢che de r√©sum√©.

Il constitue une base solide pour des projets industriels ou de recherche n√©cessitant une **adaptation rapide et peu co√ªteuse** de mod√®les de grande taille.

